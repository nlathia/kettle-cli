# operator

Operator is a command line tool for creating and shipping machine learning deployments. You can use it to generate and deploy Google [Cloud Functions](https://cloud.google.com/functions) or [Cloud Run](https://cloud.google.com/run) containerised applications.

This CLI has two primary commands:

* `operator create <name>` creates a directory containing all the boiler plate code that you need to get going. You just need to fill in the functions that loads the model and returns a prediction.
* `operator deploy <path>` deploys the code in that directory as a Cloud Function or Cloud Run application. 

This is a pre-release alpha version of this tool. Please send any bugs or feedback.

## Installing

Operator relies on you having a few things installed already. Specifically:

1. The [gcloud](https://cloud.google.com/sdk/gcloud) SDK, so that you can deploy functions to GCP.
2. Both [pyenv](https://github.com/pyenv/pyenv) and [pyenv-virtualenv](https://github.com/pyenv/pyenv-virtualenv): the boiler plate generated by this tool enables you to easily create Python virtual environments, but assumes you have these tools installed already.
3. [Optional] [Docker](https://docs.docker.com/get-docker/) to build and run Cloud Run containerized applications locally. If you are only going to be working with Cloud Functions, you can skip this step.

### Using brew

You can install `operator` using `brew` and [the operatorai tap](https://github.com/operatorai/homebrew-tap).

```bash
❯ brew tap operatorai/tap
❯ brew install operator
```

## Usage

Set up the CLI tool using `operator init`. It will take you through a one-off set up:

```bash
❯ operator init
[...]
Use the arrow keys to navigate: ↓ ↑ → ← 
? Deployment type: 
  ▸ Google Cloud Function
    Google Cloud Run
```

Create a new deployment with `operator create`:

```bash
❯ operator create service.hello-world
```

This will create a directory called (in this example) `service.hello-world` and it will add all the boiler plate you need to get going. For details about the generated boiler plate, see below.

To get started, use the `make install` command which will create a `pyenv` virtual environment for your deployment.

```bash
❯ cd service.hello-world
❯ make install # To create a pyenv-virtualenv
```
Launch it locally:

```bash
❯ make localhost
```
... and, when you're ready, deploy it!

```bash
❯ operator deploy .
```

## Generated boiler plate

The boiler plate that is generated is very similar across Cloud Run and Cloud Functions. The main difference is that Cloud Run comes with a `Dockerfile`, and has slightly different syntax in the `main.py` file.

An overview of the files that are shared in the template:

```
.
├── Makefile
├── README.md
├── bin  # Scripts that are used by the Makefile commands
│   ├── _config.sh
│   ├── cleanup.sh
│   ├── launch.sh
│   ├── remove_pyenv.sh
│   └── setup_pyenv.sh
├── main.py
├── model
│   ├── __init__.py
│   ├── artifacts # Store your serialised models in this directory
│   │   ├── README.md
│   │   ├── __init__.py
│   │   └── files.py
│   └── model.py
├── operator.config
├── requirements-dev.txt # Requirements that are needed for development
├── requirements.txt # Requirements that are needed for the production deployment
└── tests # Unit tests!
    ├── __init__.py
    └── model
        ├── __init__.py
        ├── artifacts
        │   ├── __init__.py
        │   └── test_files.py
        └── test_model.py
```

## Limitations

There are many! This is a version 0.

* Only http-triggered cloud functions are supported
* This assumes that GCP APIs have been enabled and may not fail gracefully
* There is no support for AWS yet

Please report any bugs or issues to me (neal.lathia@gmail.com) or by raising an issue in this repo.

## Notes

This tool has been built using the [Cobra Generator](https://github.com/spf13/cobra/blob/master/cobra/README.md#cobra-generator).

To add a new command:

```bash
❯ cobra add <command-name>
```
